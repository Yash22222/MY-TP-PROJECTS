{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸš€ðŸ’«Supercharge KerasNLP Models with Wandb**"
      ],
      "metadata": {
        "id": "JhUXzm3OTSWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic similarity refers to the task of determining the degree of similarity between two\n",
        "sentences in terms of their meaning. We already saw in [this](https://keras.io/examples/nlp/semantic_similarity_with_bert/)\n",
        "example how to use SNLI (Stanford Natural Language Inference) corpus to predict sentence\n",
        "semantic similarity with the HuggingFace Transformers library. In this tutorial we will\n",
        "learn how to use [KerasNLP](https://keras.io/keras_nlp/), an extension of the core Keras API,\n",
        "for the same task. Furthermore, we will discover how KerasNLP effectively reduces boilerplate\n",
        "code and simplifies the process of building and utilizing models. For more information on KerasNLP,\n",
        "please refer to [KerasNLP's official documentation](https://keras.io/keras_nlp/).\n",
        "\n",
        "Weights&Biases is an amazing platform for Experiment Tracking. However it doesn't support Multi backend Keras Core yet. Kudos to [Soumik Rakshit](https://www.kaggle.com/soumikrakshit) for his endeavour [**Wandb-addons**](https://geekyrakshit.dev/wandb-addons/) which provides multi-backend compatible Keras Callbacks.\n",
        "\n",
        "<div style=\"\n",
        "           display:fill;\n",
        "           border:solid;\n",
        "           border-radius:5px;\n",
        "           font-size:110%;\n",
        "           font-family:Verdana;\n",
        "           letter-spacing:0.5px;\n",
        "            border-style:solid;\">\n",
        "\n",
        "<h3 style=\"padding: 10px;text-align: center;\"> Outline </h3></div>\n",
        "\n",
        "1. **Getting started with KerasNLP**\n",
        "2. **Overview of dataset**\n",
        "3. **Establishing baseline with BERT.**\n",
        "4. **Improving baseline by tweaking learning rate**\n",
        "5. **Improving baseline further with learning rate scheduler**\n",
        "6. **Choosing right hyperparameters with Wandb Sweep**\n",
        "\n",
        "<div style=\"\n",
        "           display:fill;\n",
        "           border:solid;\n",
        "           border-radius:5px;\n",
        "           font-size:110%;\n",
        "           font-family:Verdana;\n",
        "           letter-spacing:0.5px;\n",
        "            border-style:solid;\">\n",
        "\n",
        "<h3 style=\"padding: 10px;text-align: center;\"> 1. Getting started with KerasNLP </h3></div>\n",
        "\n",
        "The following guide uses [Keras Core](https://keras.io/keras_core/) to work in\n",
        "any of `tensorflow`, `jax` or `torch`. Support for Keras Core is baked into\n",
        "KerasNLP, simply change the `KERAS_BACKEND` environment variable below to change\n",
        "the backend you would like to use. We select the `jax` backend below, which will\n",
        "give us a particularly fast train step below."
      ],
      "metadata": {
        "id": "o7CEj3lBq9Dv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eLhq2VbSS0D5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb3e3426-cd57-4acb-c74b-106149d26dc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q keras-nlp wandb\n",
        "!pip install --upgrade -q git+https://github.com/soumik12345/wandb-addons"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras_core as keras\n",
        "import keras_nlp\n",
        "import tensorflow_datasets as tfds\n",
        "import wandb\n",
        "from wandb_addons.keras import WandbMetricsLogger\n",
        "\n",
        "PROJECT_NAME = \"keras-nlp-x-wandb\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rgMiG1frB8o",
        "outputId": "7dd62aaa-7f09-47ad-90f3-c84905bc6114"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using JAX backend.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wAilw2irjsC",
        "outputId": "8c582203-b079-49af-dbb0-d2583cd2a278"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myash22222\u001b[0m (\u001b[33mtechtitans\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load the SNLI dataset, we use the tensorflow-datasets library, which\n",
        "contains over 550,000 samples in total. However, to ensure that this example runs\n",
        "quickly, we use only 20% of the training samples.\n",
        "\n",
        "<div style=\"\n",
        "           display:fill;\n",
        "           border:solid;\n",
        "           border-radius:5px;\n",
        "           font-size:110%;\n",
        "           font-family:Verdana;\n",
        "           letter-spacing:0.5px;\n",
        "            border-style:solid;\">\n",
        "\n",
        "<h3 style=\"padding: 10px;text-align: center;\"> 2. Overview of dataset </h3></div>\n",
        "\n",
        "\n",
        "Every sample in the dataset contains three components: `hypothesis`, `premise`,\n",
        "and `label`. epresents the original caption provided to the author of the pair,\n",
        "while the hypothesis refers to the hypothesis caption created by the author of\n",
        "the pair. The label is assigned by annotators to indicate the similarity between\n",
        "the two sentences.\n",
        "\n",
        "The dataset contains three possible similarity label values: Contradiction, Entailment,\n",
        "and Neutral. Contradiction represents completely dissimilar sentences, while Entailment\n",
        "denotes similar meaning sentences. Lastly, Neutral refers to sentences where no clear\n",
        "similarity or dissimilarity can be established between them."
      ],
      "metadata": {
        "id": "PjUXrHtvr1Q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "snli_train = tfds.load(\"snli\", split=\"train[:20%]\")\n",
        "snli_val = tfds.load(\"snli\", split=\"validation\")\n",
        "snli_test = tfds.load(\"snli\", split=\"test\")\n",
        "\n",
        "# Here's an example of how our training samples look like, where we randomly select\n",
        "# four samples:\n",
        "sample = snli_test.batch(4).take(1).get_single_element()\n",
        "sample"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEhn7SNqrzBc",
        "outputId": "64631775-8449-4b5f-89e0-a1250c391e22"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'hypothesis': <tf.Tensor: shape=(4,), dtype=string, numpy=\n",
              " array([b'A girl is entertaining on stage',\n",
              "        b'A group of people posing in front of a body of water.',\n",
              "        b\"The group of people aren't inide of the building.\",\n",
              "        b'The people are taking a carriage ride.'], dtype=object)>,\n",
              " 'label': <tf.Tensor: shape=(4,), dtype=int64, numpy=array([0, 0, 0, 0])>,\n",
              " 'premise': <tf.Tensor: shape=(4,), dtype=string, numpy=\n",
              " array([b'A girl in a blue leotard hula hoops on a stage with balloon shapes in the background.',\n",
              "        b'A group of people taking pictures on a walkway in front of a large body of water.',\n",
              "        b'Many people standing outside of a place talking to each other in front of a building that has a sign that says \"HI-POINTE.\"',\n",
              "        b'Three people are riding a carriage pulled by four horses.'],\n",
              "       dtype=object)>}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing\n",
        "\n",
        "In our dataset, we have identified that some samples have missing or incorrectly labeled\n",
        "data, which is denoted by a value of -1. To ensure the accuracy and reliability of our model,\n",
        "we simply filter out these samples from our dataset."
      ],
      "metadata": {
        "id": "linB7-DNsOdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_labels(sample):\n",
        "    return sample[\"label\"] >= 0"
      ],
      "metadata": {
        "id": "IBzGI2mIsI95"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a utility function that splits the example into an `(x, y)` tuple that is suitable\n",
        "for `model.fit()`. By default, `keras_nlp.models.BertClassifier` will tokenize and pack\n",
        "together raw strings using a `\"[SEP]\"` token during training. Therefore, this label\n",
        "splitting is all the data preparation that we need to perform."
      ],
      "metadata": {
        "id": "Vd5c9qN1s17H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_labels(sample):\n",
        "    x = (sample[\"hypothesis\"], sample[\"premise\"])\n",
        "    y = sample[\"label\"]\n",
        "    return x, y\n",
        "\n",
        "\n",
        "train_ds = (\n",
        "    snli_train.filter(filter_labels)\n",
        "    .map(split_labels, num_parallel_calls=tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "val_ds = (\n",
        "    snli_val.filter(filter_labels)\n",
        "    .map(split_labels, num_parallel_calls=tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "test_ds = (\n",
        "    snli_test.filter(filter_labels)\n",
        "    .map(split_labels, num_parallel_calls=tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "def get_batched_dataset(batch_size):\n",
        "    train_set = train_ds.batch(batch_size)\n",
        "    val_set = val_ds.batch(batch_size)\n",
        "    test_set = test_ds.batch(batch_size)\n",
        "    return train_set, val_set, test_set"
      ],
      "metadata": {
        "id": "TyPxfrIWs2rk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"\n",
        "           display:fill;\n",
        "           border:solid;\n",
        "           border-radius:5px;\n",
        "           font-size:110%;\n",
        "           font-family:Verdana;\n",
        "           letter-spacing:0.5px;\n",
        "            border-style:solid;\">\n",
        "\n",
        "<h3 style=\"padding: 10px;text-align: center;\"> 3. Establishing baseline with BERT </h3></div>\n",
        "\n",
        "\n",
        "\n",
        "We use the BERT model from KerasNLP to establish a baseline for our semantic similarity\n",
        "task. The `keras_nlp.models.BertClassifier` class attaches a classification head to the BERT\n",
        "Backbone, mapping the backbone outputs to a logit output suitable for a classification task.\n",
        "This significantly reduces the need for custom code.\n",
        "\n",
        "KerasNLP models have built-in tokenization capabilities that handle tokenization by default\n",
        "based on the selected model. However, users can also use custom preprocessing techniques\n",
        "as per their specific needs. If we pass a tuple as input, the model will tokenize all the\n",
        "strings and concatenate them with a `\"[SEP]\"` separator.\n",
        "\n",
        "We use this model with pretrained weights, and we can use the `from_preset()` method\n",
        "to use our own preprocessor. For the SNLI dataset, we set `num_classes` to 3."
      ],
      "metadata": {
        "id": "R-R3jZCotGV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%wandb\n",
        "\n",
        "with wandb.init(project=PROJECT_NAME, name=\"baseline\") as run:\n",
        "    bert_classifier = keras_nlp.models.BertClassifier.from_preset(\n",
        "        \"bert_tiny_en_uncased\", num_classes=3\n",
        "    )\n",
        "    train_set, val_set, test_set = get_batched_dataset(512)\n",
        "    bert_classifier.fit(train_set, validation_data=val_set, epochs=1, callbacks=[WandbMetricsLogger(log_freq=\"batch\")])\n",
        "    bert_classifier.evaluate(test_set, callbacks=[WandbMetricsLogger(log_freq=\"batch\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "LqL4UABRs9n0",
        "outputId": "45ae8f0e-c0d1-4c11-f387-4f3fb833b639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230923_155159-t0zqkeos</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<wandb.jupyter.IFrame at 0x7b24bc7c7c70>"
            ],
            "text/html": [
              "<iframe src='https://wandb.ai/techtitans/keras-nlp-x-wandb/runs/t0zqkeos?jupyter=true' style='border:none;width:100%;height:420px;'></iframe>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/techtitans/keras-nlp-x-wandb' target=\"_blank\">https://wandb.ai/techtitans/keras-nlp-x-wandb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/techtitans/keras-nlp-x-wandb/runs/t0zqkeos' target=\"_blank\">https://wandb.ai/techtitans/keras-nlp-x-wandb/runs/t0zqkeos</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"\n",
        "           display:fill;\n",
        "           border:solid;\n",
        "           border-radius:5px;\n",
        "           font-size:110%;\n",
        "           font-family:Verdana;\n",
        "           letter-spacing:0.5px;\n",
        "            border-style:solid;\">\n",
        "\n",
        "<h3 style=\"padding: 10px;text-align: center;\"> 4. Improving baseline by tweaking learning rate </h3></div>"
      ],
      "metadata": {
        "id": "FFhIbWndtO-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%wandb\n",
        "\n",
        "with wandb.init(project=PROJECT_NAME, name=\"change-lr-bs\") as run:\n",
        "    bert_classifier = keras_nlp.models.BertClassifier.from_preset(\n",
        "        \"bert_tiny_en_uncased\", num_classes=3\n",
        "    )\n",
        "    bert_classifier.compile(\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        optimizer=keras.optimizers.Adam(5e-5),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
        "    )\n",
        "\n",
        "    train_set, val_set, test_set = get_batched_dataset(512)\n",
        "\n",
        "    bert_classifier.fit(train_set, validation_data=val_set, epochs=1, callbacks=[WandbMetricsLogger(log_freq=\"batch\")])\n",
        "\n",
        "    bert_classifier.evaluate(test_set, callbacks=[WandbMetricsLogger(log_freq=\"batch\")])"
      ],
      "metadata": {
        "id": "Et9NKD_LtBKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just tweaking the learning rate alone was not enough to boost performance. Let's try again, but this time with\n",
        "`keras.optimizers.AdamW`, and a learning rate schedule."
      ],
      "metadata": {
        "id": "8cQh6xKUtvUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"\n",
        "           display:fill;\n",
        "           border:solid;\n",
        "           border-radius:5px;\n",
        "           font-size:110%;\n",
        "           font-family:Verdana;\n",
        "           letter-spacing:0.5px;\n",
        "            border-style:solid;\">\n",
        "\n",
        "<h3 style=\"padding: 10px;text-align: center;\"> 5. Improving baseline further with learning rate scheduler </h3></div>"
      ],
      "metadata": {
        "id": "F47F-60gt5SS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TriangularSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \"\"\"Linear ramp up for `warmup` steps, then linear decay to zero at `total` steps.\"\"\"\n",
        "\n",
        "    def __init__(self, rate, warmup, total):\n",
        "        self.rate = rate\n",
        "        self.warmup = warmup\n",
        "        self.total = total\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\"rate\": self.rate, \"warmup\": self.warmup, \"total\": self.total}\n",
        "        return config\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = keras.ops.cast(step, dtype=\"float32\")\n",
        "        rate = keras.ops.cast(self.rate, dtype=\"float32\")\n",
        "        warmup = keras.ops.cast(self.warmup, dtype=\"float32\")\n",
        "        total = keras.ops.cast(self.total, dtype=\"float32\")\n",
        "\n",
        "        warmup_rate = rate * step / self.warmup\n",
        "        cooldown_rate = rate * (total - step) / (total - warmup)\n",
        "        triangular_rate = keras.ops.minimum(warmup_rate, cooldown_rate)\n",
        "        return keras.ops.maximum(triangular_rate, 0.0)"
      ],
      "metadata": {
        "id": "5vBxGiwVtv-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%wandb\n",
        "\n",
        "with wandb.init(project=PROJECT_NAME, name=\"lr-schedule\") as run:\n",
        "    bert_classifier = keras_nlp.models.BertClassifier.from_preset(\n",
        "        \"bert_tiny_en_uncased\", num_classes=3\n",
        "    )\n",
        "\n",
        "    train_set, val_set, test_set = get_batched_dataset(512)\n",
        "    # Get the total count of training batches.\n",
        "    # This requires walking the dataset to filter all -1 labels.\n",
        "    epochs = 3\n",
        "    total_steps = sum(1 for _ in train_ds.as_numpy_iterator()) * epochs\n",
        "    warmup_steps = int(total_steps * 0.2)\n",
        "\n",
        "    bert_classifier.compile(\n",
        "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        optimizer=keras.optimizers.AdamW(\n",
        "            TriangularSchedule(1e-4, warmup_steps, total_steps)\n",
        "        ),\n",
        "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    )\n",
        "\n",
        "    bert_classifier.fit(train_set, validation_data=val_set, epochs=epochs, callbacks=[WandbMetricsLogger(log_freq=\"batch\")])\n",
        "    bert_classifier.evaluate(test_set, callbacks=[WandbMetricsLogger(log_freq=\"batch\")])"
      ],
      "metadata": {
        "id": "X21_MCEquAy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"\n",
        "           display:fill;\n",
        "           border:solid;\n",
        "           border-radius:5px;\n",
        "           font-size:110%;\n",
        "           font-family:Verdana;\n",
        "           letter-spacing:0.5px;\n",
        "            border-style:solid;\">\n",
        "\n",
        "<h3 style=\"padding: 10px;text-align: center;\"> 6. Choosing right hyperparameters with Wandb Sweep </h3></div>"
      ],
      "metadata": {
        "id": "X2fSs9BnuERD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "sweep_config = {\n",
        "    'project': PROJECT_NAME,\n",
        "    'method': 'grid',\n",
        "    'run_cap': 6,\n",
        "    'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'\n",
        "    },\n",
        "    'parameters': {\n",
        "\n",
        "        'learning_rate': {\n",
        "            'values': [5e-6, 2e-5, 5e-5, 1e-4]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [256, 512]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "sweep_defaults = {\n",
        "    'learning_rate': 5e-5,\n",
        "    'batch_size': 512,\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=PROJECT_NAME)"
      ],
      "metadata": {
        "id": "lfHJtapZuE8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "\n",
        "    wandb.init(project=PROJECT_NAME, config=sweep_defaults)\n",
        "\n",
        "    bert_classifier = keras_nlp.models.BertClassifier.from_preset(\n",
        "      \"bert_tiny_en_uncased\", num_classes=3)\n",
        "\n",
        "    train_set, val_set, test_set = get_batched_dataset(wandb.config.batch_size)\n",
        "\n",
        "    optimizer = keras.optimizers.AdamW(learning_rate = wandb.config.learning_rate,\n",
        "                                        epsilon = 1e-8)\n",
        "\n",
        "    bert_classifier.compile(\n",
        "          loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "          optimizer=optimizer,\n",
        "          metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
        "    )\n",
        "\n",
        "    epochs = 2\n",
        "\n",
        "    bert_classifier.fit(train_set, validation_data=val_set, epochs=epochs,\n",
        "              callbacks=[WandbMetricsLogger(log_freq=\"batch\")])\n",
        "\n",
        "    bert_classifier.evaluate(test_set, callbacks=[WandbMetricsLogger(log_freq=\"batch\")])"
      ],
      "metadata": {
        "id": "4RkQGOnduKJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.agent(sweep_id, function=train)"
      ],
      "metadata": {
        "id": "KXxX4nH9uPaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%wandb shivance/\"keras-nlp-x-wandb\"/reports/Vmlldzo1Mjk1ODQ4"
      ],
      "metadata": {
        "id": "6lsF_3hJuST0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We hope this tutorial has been helpful in demonstrating the ease and effectiveness\n",
        "of using KerasNLP and BERT for semantic similarity tasks.\n",
        "\n",
        "Throughout this tutorial, we demonstrated how to use a pretrained BERT model to\n",
        "establish a baseline and improve performance by training a larger RoBERTa model\n",
        "using just a few lines of code.\n",
        "\n",
        "The KerasNLP toolbox provides a range of modular building blocks for preprocessing\n",
        "text, including pretrained state-of-the-art models and low-level Transformer Encoder\n",
        "layers. We believe that this makes experimenting with natural language solutions\n",
        "more accessible and efficient."
      ],
      "metadata": {
        "id": "S2Ajneg5ucKt"
      }
    }
  ]
}